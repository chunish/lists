Machine learning
Warming up
几种不同矩阵的积：
点积Dot product (Matmul product): 
[■(x_11&x_12&x_13@x_21&x_22&x_23@x_31&x_32&x_33 )]  ∙ [■(y_11&y_12&y_13@y_21&y_22&y_23@y_31&y_32&y_33 )]= ∑_(i=1)^3▒∑_(j=1)^3▒〖x_ij y_ij 〗
Element-wise (Hadamard) product:
[■(x_11&x_12&x_13@x_21&x_22&x_23@x_31&x_32&x_33 )]  ⨀ [■(y_11&y_12&y_13@y_21&y_22&y_23@y_31&y_32&y_33 )]= [■(x_11 y_11&x_12 y_12&x_13 y_13@x_21 y_21&x_22 y_22&x_23 y_23@x_31 y_31&x_32 y_32&x_33 y_33 )]
Kronecker product:
 

奥克姆剃刀：
在所有可能选择的模型中，能够很好地解释已知数据并十分简单才是最好的模型，也就是最应该选择的模型。——在同样能够解释已知观测现象的假设中，我们应该选择“最简单”的那一个。

没有免费午餐定理：
没有一个机器学习算法，总是比其他的更好。

范数：
先看公式定义，对于向量x的p范数为：
‖x‖_p= (∑_i▒|x_i |^p )^(1/p)

从意义上讲，范数表示任意空间某点到该空间零点的距离，并满足以下条件：
	||X || ≥ 0
	||αX|| = |α|﹒||X||，其中α为实数
	||X + Y|| ≤ ||X|| + ||Y||

其中L1距离也称作曼哈顿距离，L2距离也称作欧氏距离，岭回归添加了L2正则化，LASSO回归添加了L1正则项
切比雪夫距离（Chebyshev Distance）：
D_Chebyshev (X,Y) = max(|x_i  - y_i | )   x_i,y_i ϵ X,Y


谈空间前需要先谈对距离的定义，实际上的距离除了常用的直线距离外，还有向量距离、函数距离、曲面距离等。距离就是一个抽象的概念：
设Ω是任意一个非空集合，对于Ω中的任意两点a和b，有一实数D(a, b)与之对应，且满足：

	D(a, b) ≥ 0，当且仅当a = b时D(a, b) = 0
	D(a, b) ≡ D(b, a)
	D(a, b) ≤ D(a, x) + D(x, b)

则称D(a, b)是Ω中的一个距离。



范数的集合  赋范空间  线性结构  线性赋范空间
距离的集合  度量空间  线性结构  线性度量空间

线性赋范空间  内积运算  内积空间
内积空间  有限  欧几里得空间

内积空间  完备  希尔伯特空间

完备性：空间中的极限运算不能超出空间范围，无论进行什么计算，结果都在此空间内。

函数空间的概念，可以通过另外一种角度来看：
泰勒级数展开，可以看成是一个以{x^i }_0^∞作为基底的空间
傅里叶级数展开，可以看成是一个以三角函数为基底的空间

这两种角度看，一个函数这样的级数展开被视为希尔伯特空间
核函数
定义一个函数空间中无穷维矩阵K(x, y)如果满足：
正定性，对于任何的函数f(x)都有：
∬▒〖f(x)∙K(x,y)∙f(y)dxdy ≥ 0〗
对称性，K(x, y) = K(y, x)
∫▒〖K(x,y)〗∙ψ(x)dx = λψ(x)

则这个二维函数K(x, y)称为核函数

再生希尔伯特空间RKHS (Reproducing Kernel Hilbert Space)
RKHS是由kernel构成的空间
再生性：将计算内积需要的无穷维的积分，转换成只需要计算kernel函数就好了。

线性空间
线性空间：空间中总有一组基底可以通过线性方式描述空间中的任何点
Hilbert空间：
再生Hilbert空间：

判别模型与生成模型
判别模型：给定X，通过直接建模P(c│X)来预测c，这样的模型叫做判别式模型
生成模型：先对联合概率分布P(c,X)建模，然后再由此获得P(c│X)，这样得到的是生成式模型

模型的评价
Recall:用户感兴趣的信息中有多少被检索出来
Precision:检索出来的信息中有多少是用户感兴趣的
相似性度量
余弦相似度：
cos⁡〖(x,y)= (x∙y)/(‖x‖_2∙‖y‖_2 )〗
协方差：衡量两个随机分布XY之间的总体变化性，也常用来衡量两个随机变量的线性相关性。
Cov(X,Y) = E[(X - E[X] )(Y - E[Y] ) ]
皮尔逊相关系数：
ρ_XY  =  Cov(X,Y)/(√(D[X] )∙√(D[Y] ))
特征分解：
奇异分解（SVD）：
互信息：

边际分布：
对于离散随机变量X∈ Ω_x  ,Y∈ Ω_y，其联合概率分布满足
∑_(x∈ Ω_x)▒∑_(y∈Ω_y)▒P(x_i,y_i )   = 1,且P(x_i,y_i )  ≥ 0
对于联合概率分布p(x, y)，可以分别对x和y进行求和：
对于固定的x：
∑_(y∈Ω_y)▒P(x,y)   = P(X=x)  = p(x)
对于固定的y:
∑_(x∈Ω_x)▒P(x,y)   = P(Y=y)  = p(y)
这里p(x), p(y)就成为p(x, y)的边际分布。


KL(K)散度（相对熵）：两个概率分布P和Q差别的非对称性相似性度量。KL散度是用来度量使用基于Q的编码。典型情况下，P表示数据的真实分布，Q表示数据的理论分布，或者P的近似分布。值为非负。
KL(P||Q) = ∑_(i=1)^N▒p_i  ln p_i/q_i   = H(P) + H(P,Q)
KL≥0，当且仅当P=Q时， KL = 0
所谓非对称性：KL(P||Q) ≠ KL(Q||P)
JS散度(Jensen-Shannon):
JS散度度量了两个概率分布的相似度，基于KL散度的变体，解决了KL的非对称问题。一般JS取值0到1之间。定义如下：
JS(P‖Q┤ )  =  1/2 KL(P‖ (P + Q)/2┤ )  +  1/2 KL( Q‖(P + Q)/2┤ )
如果两个分配P,Q离得很远，完全没有重叠的时候，KL散度值是没有意义的，而JS散度值是一个常数。这在学习算法中是比较致命的，这就意味这这一点的梯度为0。梯度消失了。
F散度（f-divergence）：
Wasserstein距离(EMD: Earth Mover's Distance)：
Wasserstein距离是度量两个分布之间的距离。定义如下：
W(P,Q)  =   〖inf〗┬(γ∼Π(P,Q) )⁡〖E_((x,y)~γ) 〗 [‖x - y‖ ]

Wasserstein距离相比KL散度和JS散度的优势在于：即使两个分布的支撑集没有重叠或者重叠非常少，仍然能反映两个分布的远近。而JS散度在此情况下是常量，KL散度可能无意义。
Wasserstein距离不仅告诉两个分布之间的距离，而且能够告诉我们它们具体如何不一样，即如何从一个分布转化为另一个分布。如下图所示，Wasserstein能够告诉我们每一份probability density的转移方案。
 
这个转化过程还可以做成一个连续的过程，可以把A分布连续转化为B分布，并且这个转化过程是能够保持其几何特征的，如下面两图所示。
参考http://www.stat.cmu.edu/~larry/=sml/Opt.pdf
过拟合与欠拟合
链式法则：

过拟合：
欠拟合：
解决欠拟合的方法：
1. 增加模型复杂度
2. 减小正则项系数

解决过拟合的方法：
1. 交叉验证
2. 降低模型复杂度
3. 添加或增大正则项惩罚
4. 使用更丰富的训练数据
5. early stop
6. dropout
不平衡数据处理方法
1. 扩充数据集，尤其是稀少样本类的补充
2. Re-sampling: over-sampling和under-sampling。前者是对小类数据样本重采样的量大于该样本的总量，即采样量大于样本量；后者则是对大类数据样本进行重采样，而使该类的采样量小于样本量。
3. 合成数据，制造数据来补充匮乏样本
4. 改变算法：对小样本类添加奖励项，对大样本类添加惩罚项 




统计学习方法三要素：模型、算法、策略
最小二乘法：在线性回归中，最小二乘法就是试图找到一条直线，使所有样本到该直线的欧氏距离之和最小。
LDA[Fisher 判别]:给定训练集，设法将集合中每一个样本投影到一条直线上，使同类的样本投影尽可能靠近，不同样本投影尽可能远离。在对新样本进行分类时，先将新样本投影到该直线上，再根据投影点的位置进行分类。
 
最大似然估计：
对未知样本集D分布的评估时,先假设D符合某一分布规律(如正态分布N(μ,σ^2))，分布的参数未知，如何求解出这些参数呢？随机在D里抽取n个样本，然后根据这n个样本统计来估计均值。由于抽取事件是相互独立的，所以概率为：L(θ)=L(x_1,x_2,x_3,…,x_n;θ)= ∏_(i=1)^n▒〖p(x_i│θ) 〗，分别对等式取对数: H(θ)=ln⁡L(θ)= ∑_(i=1)^n▒ln⁡〖p(x_i│θ)〗 ，若使L最大，θ ̂=argmax L(θ)，分别对θ里面的参数求偏导建立微分方程组，即可求解。

矩估计
大数定理：
如果总体分布X对于任意的ε>0，使
lim┬(n→∞)⁡〖P(1/n ∑_(i=1)^n▒〖X_i^k- E[X^k ] 〗≥ε)= 0〗
这样说明，样本容量n非常大时，样本的k阶矩与总体k阶矩相差很小。矩估计方法就是借此，使用样本k阶矩来替代总体k阶矩。

矩：A(a,k)=E〖(x-a)〗^k表示随机分布X在a点的K阶矩。若a=0则称为原点矩。

一般一阶期望，二阶方差。
AdaGrad一阶矩，RMSProp二阶矩，Adam一阶、二阶配合使用。
拉格朗日乘子法：

梯度消失与梯度爆炸
Classification
SVM: 2-classes classification
SVM是一种二分类模型，基本模型是定义在特征空间上使间隔最大化的分类器。可以形式化为一个求解凸二次优化问题，也等价于（L2）正则化的合页损失函数最小化问题。
合页损失函数：置0的出发点：非支持向量（置信度（函数间隔）大于1，且被正确分类）不参与loss计算。
核技巧：通过一个非线性变换将输入空间映射到另外一个空间，然后在新的空间上用线性分类学习方法学习分类模型。
函数间隔：
几何间隔：
 

支持向量：样本点中与分离超平面最接近的样本点的实例集合
在决定分离超平面时，只有支持向量起作用，其他实例点不起作用
硬间隔
软间隔
\ 
SMO

Kernel使用的比较：

Kernel	优点	缺点
线性核	泛化性能好，计算速度快，一般优先考虑
	仅限于应用在线性可分的情况下
多项式核	可解决非线性问题	参数多，难调
RBF核	分类更优，可以得到复杂的边界，参数少，只有一个σ，解决非线性问题首选
	解释性差，无法得知各个特征的权重，易过拟合，需要严控σ（gauss平坦程度）

SVM缺点：
对噪声敏感
样本量不宜太大
对多分类问题存在困难
Logistic Regression
 
Naïve Bayes
P(A│B)= P(AB)/P(B) 

P(x_1,x_2,x_3, …,x_N )=P(x_1 ) ∏_(i=2)^N▒P(x_i ├|x_1,〖…,x〗_(i-1) ┤ ) 

P(A)= ∑_(i=1)^N▒〖P(A│B_i ) 〗 P(B_i )

P(B_i│A)= (P(A│B_i )P(B_i ))/(∑_(j=1)^N▒〖P(A│B_j )P(B_j ) 〗)

Decision Tree
分类决策树是一种描述基于特征对实例进行分类的树形结构。本质上是从训练数据中归纳出一组分类（回归）规则。
可以认为是if-then规则的集合，由决策树的根节点到叶节点的每条路径构建成一条规则，叶节点的类对应着规则的结论，每一个实例都被一条路径所覆盖，互斥且完备。也可以认为是定义在特征空间与类空间上的条件概率分布。
主要优点是：
模型具有可读性，分类速度快
生成步骤：特征选择、构建决策树、剪枝
特征选择：
信息增益：得知特征X的信息而使得类Y的信息不确定性减少的程度：训练集经验熵– 给定的条件熵。
信息增益率：实际情况下，以信息增益作为评价标准，易倾向于选择样可取值较多（子节点更多）的特征，信息增益率因此而生：特征X的信息增益/特征X值的熵
基尼指数：假设训练集中共有K个类，样本点属于第i类的概率为p_i，则概率分布的Gini指数为：
Gini(p)= ∑_(i=1)^K▒p_i  (1-p_i )= 1-∑_(i=1)^K▒p_i^2 
选择方法：
对训练数据集D（或子集D_i），计算其每个特征的筛选方法（信息增益、信息增益率、Gini指数等），选择出最佳特征作为当前根节点，然后将数据集根据该特征分配到子节点，依次递归，直至叶节点
剪枝：最小化loss函数（经验熵+ L1正则项，拟合度+复杂度），L1是为了减小模型负责度，当α确定时，选择loss最小的模型

分类树和回归树的区别：
	在于样本输出，若是离散就是分类树，若是连续值则为回归树
	连续值的处理方法不同。分类树如上所述用Gini来分割，而回归树则通过计算平方差损失
	建立后做预测的方式不同：分类树输出叶子节点里概率值最大的类别，回归树输出最终叶子节点的均值或者中位值来作为预测值

CART回归树，相比于分类决策树中的熵，RT选择通过最小平方差来进行树的分裂。对于任意划分的特征A，对应的划分点s将数据集D分割成D1和D2，求D1和D2各自的方差之和最小时所对应的划分点
 
其中c1和c2分别为D1和D2的均值

Boosting Tree
Boosting tree = Boosting + CART，处理分类与回归问题，采用加法模型，使用boosting的方法，在设定迭代次数的情况下，递归地根当前模型的残差来拟合新的树，组成一个加法模型，形成一个强学习器。关注被已有学习器漏算的那部分来获取新的树。

Gradient Boost Decision Tree
跟其他boosting方法一样，gradient boosting也是通过一些“弱学习器”来组合成一个健壮的“强学习器”。

(f_t ) ̂= f ̂_(t-1)+ ξg_t
其中ζ为学习率，且
g_t= -∂Loss(f_(t-1),f_t )/(∂f_(t-1) )

每一次计算都是为了减少上一次的残差，为了消除残差，便在残差减小的梯度方向建立模型。
利用loss function（平方损失）的负梯度在当前模型的值作为回归问题提升树的残差的近似值，将残差建立在梯度下降的方向上，拟合一个回归树。

XGBoost
特点：
	不同于GBDT对loss只使用了一阶近似，XGBoost对Loss进行泰勒二阶展开近似，提高了精度
	XGBoost在Loss上添加了正则项，包含了对复杂模型的惩罚，比如L1对叶节点的个数和L2树的深度
	使用贪心算法，分层添加决策树的叶节点

方法：
	对所有特征都按照特征的数值进行pre-sorted
	在遍历特征的时候用O的代价找到一个特征上最好分割的点
	在找到的分割点后，将数据分割成左右子节点

缺点：
每轮迭代时，都要遍历数据集多次
空间消耗大：需要保存数据的特征值，还要保存特征排序后的结果（需要消耗训练数据两倍的内存），亦即是时间消耗也大，还包括大规模的遍历所耗费的时间
对缓存优化不好
	

Light-GBM
Light-GBM的主要特色体现在：
	使用了直方图思想，将输入样本离散化成k个整数，形成一个宽度为k的直方图，然后再根据直方图来寻找最优切分点
	在树的分裂策略上使用了leaf-wise来取代之前的level-wise，在相同迭代次数下，前者更容易收敛，因此也容易过拟合，故有个深度阈值来加以控制。
 
 
	使用了直方图差的方法，降低了计算量。一个叶节点的直方图等于其父节点与其兄弟节点之差。
 
	支持多进程计算
 
Random forest

AdaBoost
主要关注4个问题：
如何计算误差率：统计错分量的权重总和
如何计算弱学习器系数：错误率之商的对数
如何更新样本权重：指数函数
如何定义loss：指数loss
AdaBoost可以看成是：一个loss function为指数损失函数、学习算法为前向分步算法的、用来做二分类的加法模型。

 


Maximum entropy model
学习概率模型时，在所有可能的模型中，
Expectation maximization

Ensemble Learning

Bagging
Boosting

Regression
Linear regression
线性回归：
 

使用loss，MSE：
 

岭回归：L2正则
LASSO回归：L1正则

Clustering
概率图模型
Sequence Prediction
Markov chain：a state chain onlydepends on finite former states, irrespective of any state else.
 
Probabilistic Graph Model
Hidden Markov Model
 
Markov两个基本假设：
	1-gram：任意时刻t的状态只依赖于上一时刻t-1的状态，与时刻t无关
	观测独立：任意时刻的观测结果都只依赖于该时刻的Markov chain状态，与其他无关
HMM主要由：
状态转移概率矩阵A：
观测概率矩阵B：
初始状态概率向量π：
三个问题：
概率计算问题：已知Markov模型λ(A,B,π)和观测序列O(o_1,o_2,o_3,…,o_N)，计算在模型λ下序列O出现的概率P(O│λ)
学习问题：已知观测序列O(o_1,o_2,o_3,…,o_N)，估计λ(A,B,π)的参数，使得P(O│λ)最大。（即求最大似然估计问题）
预测问题（Decoding问题）：已知模型λ(A,B,π)和观测序列O(o_1,o_2,o_3,…,o_N)，求对给定观测序列条件概率P(I│λ)最大的状态序列I(i_1,i_2,i_3,…,i_N).
应用场景:
股票预测
输入法默认输入

Conditional Random Field


